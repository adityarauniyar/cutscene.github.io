<!DOCTYPE html>
<html>
<head>
  <link rel = "icon" href = "assets/title_image.png" type = "image/x-icon">
        
  <meta charset="utf-8">
  <meta name="description"
        content="cutscene: Towards Universal Visual Place Recognition ">
  <meta name="keywords" content="cutscene,FoundVLAD, Foundation Models, Visual Place Recognition, VPR, DINOv2, DINO, SAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="assets/Thumbnail.png" />        
  <title>cutscene: Towards Universal Visual Place Recognition </title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://cutscene.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title"><a href="https://visual-learning.cs.cmu.edu/">16824</a> Course Project: <br>
             Active vision for Next Best View Planning in outdoor scenes.</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://adityarauniyar.com/">Aditya Rauniyar</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/omaralama/">Omar Alama</a>,</span>
              <span class="author-block">
                <a href="https://yuechuanhou.com//">Yuechuan Hou</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/mukul-ganwal/">Mukul Ganwal</a>,</span> 
                <br>
              <!-- <span class="author-block">
              <a href="https://theairlab.org/">Sebastian Scherer</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://robotics.iiit.ac.in/">Madhava Krishna</a><sup>2</sup>,</span>  
              <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=oVS3HHIAAAAJ&hl=en">Sourav Garg</a><sup>4</sup></span> -->
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://www.cmu.edu/" style="color: rgb(179, 8, 8);">Carnegie Mellon University</a>
            </span>
          </div>          

          <!-- <div class="is-size-7 publication-authors">
            <span class="author-block">* denotes equal contribution</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://vcc.tech/UrbanScene3D"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
        <!-- <img source src="./data/method_viz/Splash GIF.gif" /> -->
        <!-- <video id="dinov2_gardens" autoplay controls muted loop playsinline height="100%"> -->
          <!-- <source src="./data/method_viz/splash_vid_compressed.mp4" -->
                  <!-- type="video/mp4"> -->
        <!-- </video> -->
      <!-- </video> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- <span class="coolname">cutscene</span> enables <span>universal visual place recognition (VPR) <i>anywhere</i>, <i>anytime</i> and under <i>anyview</i>.</span>  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->

<section class="hero is-small is-light">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this project, we extend the exploration of autonomous robotic 
            tasks in the context of larger outdoor scenes. Building upon the 
            referenced work, we focus on planning views for these expansive 
            environments, addressing questions related to optimal data collection 
            given a set of reference images. A significant contribution of our 
            approach lies in the introduction of a cutscene augmentation method. 
            This innovative technique involves semantically dividing larger 
            outdoor scenes into smaller components. Our model is then trained 
            to predict uncertainty and RGB values for novel poses within these 
            segmented scenes. This cutscene augmentation method serves a dual 
            purpose. First, it effectively reduces the size of the dataset by 
            a significant percentage, enhancing the efficiency of the training 
            process. Second, and more importantly, it substantially increases 
            the accuracy of novel view predictions. By leveraging this method, 
            our project aims to overcome challenges associated with data 
            collection in large-scale outdoor scenarios, providing a valuable 
            contribution to the broader field of autonomous robotic tasks. 
            Our experiments, using both synthetic and real-world data, 
            demonstrate the effectiveness of our proposed uncertainty-guided 
            approach. The results showcase improved accuracy in scene representations 
            compared to baseline methods, validating the utility and generalizability 
            of our methodology.
          </p>
        </div>
      </div>
    </div>
    </div>
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <a id="overview_video"></a>
          <iframe
            src="./data/">
          </iframe>
        </div>
      </div>
    </div>
  </div> -->
    <!--/ Paper video. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <a id="interactive_demo"></a>
        <h2 class="title is-3">Introduction and Related work</h2>
        <div class="content has-text-justified">
        <p> 
          (WIP) Embodied robotic intelligence relies on active perception 
          and exploration, essential for various applications like 
          robotic manipulation, inspection, and vision-based navigation. 
          The autonomous collection of data plays a pivotal role in 
          scene understanding and subsequent tasks [1]. However, a 
          significant challenge lies in efficiently planning a sequence 
          of views for sensors, ensuring the acquisition of the most 
          valuable information while adhering to platform-specific 
          constraints [2]. Addressing this challenge is crucial for 
          enhancing the training process, particularly in scenarios 
          involving larger scenes.
        </p>
        </div>
      </div>
    </div>

    

    <div class="columns is-centered">

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Multi-UAV Data Gathering</h2>
        <img id="q_image" src = "data/related_work/multi_uav_data_gathering.gif" width="500">
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Scene Understanding for Manipulation</h2>
          <img id="db_image" src = "data/related_work/nbv_manipulation.gif" width="500">
        </div>
      </div>

      <script src = "data/trajectory_data/hawkins.js"></script>
      <script src = "demo/plot.js"></script>
  
    </div>

    <p></p>
    </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <a id="interactive_demo"></a>
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
        <p> 
          (WIP) Describes the three contributions. One sentence each.
        </p>
        </div>
      </div>
    </div>

  <div class="container is-max-desktop">
  <div class="columns is-centered">

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">Training on larger outdoor scenes</h2>
      <img id="db_image" src = "data/method_viz/Selection_180_90_30.gif" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">Hierarchical training strategy</h2>
      <img id="db_image" src = "data/method_viz/cutscene.gif" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">View planning to gather NBV</h2>
      <img id="db_image" src = "data/method_viz/view_capture_180.gif" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>
  </div>

  </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <a id="interactive_demo"></a>
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
        <p> 
          (WIP) Describes the two major backgrounds on NeRF and PixelNeRF. One sentence each.
        </p>
        </div>
      </div>
    </div>

    

    <div class="columns is-centered">

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">NeRF</h2>
          <div class="NeRF-video">
            <iframe width="480" height="270" src="https://www.youtube.com/embed/JuH79E8rdKc?si=AsJpnK8WQT1vKl5q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>      
          </div>
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">PixelNeRF</h2>
          <div class="PixelNeRF-video">
            <iframe width="480" height="270" src="https://www.youtube.com/embed/voebZx7f32g?si=K-HQnVwn3tHiTIwu" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>          </div>
        </div>
      </div>

      <script src = "data/trajectory_data/hawkins.js"></script>
      <script src = "demo/plot.js"></script>
  
    </div>

    <p></p>
    </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <a id="interactive_demo"></a>
        <h2 class="title is-3">Our Approach</h2>
        <div class="content has-text-justified">
        <p> 
          (WIP) Describes the approach from 
          (1) The network architechture, 
          (2) The uncertainity estimation, 
          (3) Uncertainity guided NBV planning.
        </p>
        </div>
      </div>
    </div>

  <div class="container is-max-desktop">
  <div class="columns is-centered">

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">Network architechture</h2>
      <img id="db_image" src = "data/method_viz/network_architechture.png" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">Uncertainity Estimation</h2>
      <img id="db_image" src = "data/method_viz/loss_function.png" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>

  <div class="column has-text-centered">
    <div class="content">
      <h2 class="title is-4">NBV planning</h2>
      <img id="db_image" src = "data/method_viz/view_capture_180.gif" width="500">
      <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
    </div>
  </div>
  </div>

  </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <a id="interactive_demo"></a>
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
        <p> 
          (WIP) Describes the 
          (1) 30/60/90 without cutscene and with 60-deg cutscene method of training. 
          (2) Uncertainity calculation on uniform sampling and selection for NBV planning.
          One sentence each.
        </p>
        </div>
      </div>
    </div>

    

    <div class="columns is-centered">

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Training on larger outdoor scenes</h2>
          <img id="db_image" src = "data/method_viz/Selection_180_90_30.gif" width="500">
          <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4"></h2>
          <div class="content has-text-justified">
            <p> 
              (WIP) Describes the 
              (1) 30/60/90 without cutscene method of training.
            </p>
            </div>
        </div>
      </div>

      <script src = "data/trajectory_data/hawkins.js"></script>
      <script src = "demo/plot.js"></script>
  
    </div>

    <div class="columns is-centered">

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4">Cutscene Augmentation</h2>
          <img id="db_image" src = "data/method_viz/how_to_cutscene.gif" width="500">
          <!-- <img src = "data/viz_gifs/row3_col3.gif"> -->
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h2 class="title is-4"></h2>
          <div class="content has-text-justified">
            <p> 
              (WIP) Describes the 
              (1) 60-deg cutscene method of training.
            </p>
            </div>
        </div>
      </div>

      <script src = "data/trajectory_data/hawkins.js"></script>
      <script src = "demo/plot.js"></script>
  
    </div>

    <p></p>
    </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website adapted from the Nerfies templates, which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            If you use the <a href="https://github.com/cutscene/cutscene.github.io">source code</a> of this website,
            please also link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a> in your footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
